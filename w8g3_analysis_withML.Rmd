---
title: "Week 7"
author: "Group 3: Mara Alexeev, Parker Bannister, Mitchell Flagg"
date: "10/26/2020"
output: html_document
---

**Deliverables:**

Code available at our github repo: [MaraAlexeev/720_regex_mmp](https://github.com/MaraAlexeev/720_regex_mmp)

[Error analysis](#eval) of the top 10 false positive and false negative terms

[Model Output](#output) - same as prior week: group3_predictions_wk6.csv

[Write a paragraph](#metric) as to which metric you are trying to optimize for (sensitivity, specificity, PPV, NPV, accuracy, or F1-score)


# Libraries
```{r libraries, warning=FALSE, message=FALSE}
#get relevant libraries
library(tidyverse)
library(forcats)
#install.packages("caret")
library(caret)
#install.packages("rlang")
#install.packages("cvms")
#install.packages("cvms")
library(cvms)
#install.packages("tidyr")
library(tidyr)
library(broom)    
library(tibble)   
#install.packages("here")
library(here)
#library(tidytext)
#install.packages("ggimage")
library(ggimage)
#install.packages("rsvg")
library(rsvg)
#library(textstem)
library(text2vec)
library(data.table)

#install.packages("glmnet")
library(glmnet)
install.packages("ROCR")
library(ROCR)
library(pROC)
```

# Functions
```{r custom functions}

search_assign_by_cc <- function(data, search_terms, column_to_search) {
  pasted_match <- paste(search_terms, collapse = "|")
  
  searched_and_assigned <- data %>%
  mutate(covid_guess = as.integer(grepl(pattern = pasted_match, x = column_to_search, ignore.case = TRUE))) 
  
  return(searched_and_assigned)
}

covid_prediction_count <- function(data) {
  prediction_count <- data %>% 
  group_by(covid_guess) %>%
  summarise(count = n())
  
  return(prediction_count)
}

covid_model_comparison <- function(data_predictions, data_w_labels) {
  only_labeled_rows <- data_w_labels %>%
    filter(label == "0"| label == "1")
  
  joined_data <- only_labeled_rows %>%
  left_join(data_predictions, by = "id") 
  
  joined_data$label <- as.numeric(joined_data$label)
  
  renamed_joined_data <- joined_data %>% 
  rename(
    covid_status = label,
    covid_prediction = covid_guess
    )
  
  error_analysis <- renamed_joined_data %>%
 mutate(results =  case_when(
    covid_status == 0 & covid_prediction == 0 ~ "True Negative",
    covid_status == 1 & covid_prediction == 1 ~ "True Positive",
    covid_status == 0 & covid_prediction == 1 ~ "False Positive",
    covid_status == 1 & covid_prediction == 0 ~ "False Negative"
  )
 )
  return(error_analysis)
}

covid_model_comparison_table <- function(data_with_status_and_prediction){
  real_and_prediction <- data_with_status_and_prediction %>%
  group_by(covid_status, covid_prediction) %>%
  summarise(count = n())
  
   return(real_and_prediction)
}

evaluation_table <- function(covid_model){

model_basic_table <- data.frame("target" = c(covid_model$covid_status),
                                  "prediction"= c(covid_model$covid_prediction)) 

model_eval <- evaluate(model_basic_table,
                 target_col = "target",
                 prediction_col = "prediction",
                 type = "binomial")

return(model_eval)
}

#Function to sort results by cc

cc_by_results <- function(model, results_factor, cc_number_to_return = 10) {
  results_filtered <- model %>%
  filter(results == results_factor)
  
  top_cc <- results_filtered%>%
  group_by(results, cc1.x) %>%
  summarise(count = n()) %>% 
  arrange(desc(count)) %>%
  head(cc_number_to_return)
  

  return(print(top_cc))
}
```

## Load data files
```{r load files, warning=FALSE, message=FALSE}
#read in data and print
setwd("M:/Documents/MBI/720/720_regex_mmp/")
getwd()
covidclass_without_labels <- read.table("./data_do_not_alter/covidclass_without_labels.csv",
                                        na.strings = "", header = TRUE, sep = "\t")
#,
covidclass_w_labels <- read.table("./data_do_not_alter/covidclass_30_percent_labels.csv",
                                        na.strings = "", header = TRUE, sep = "\t")
```

## Clean Data
```{r clean data}

colnames(covidclass_without_labels)[1] <- "patientID|chief complaint|"
clean_data <- covidclass_without_labels %>%
  mutate(id_cc = `patientID|chief complaint|`) 
clean_data <- clean_data %>%
  separate(col = id_cc, into = c('id', 'cc1', 'cc2'), sep = "[|]", remove = F)

clean_data$cc1[which(clean_data$cc1 == "")] = "No CC provided"

clean_data$cc1 <- as_factor(clean_data$cc1)
colnames(covidclass_w_labels)[1] <- "patientID|labels|chief complaint|"
clean_labeled_data <- covidclass_w_labels %>%
  mutate(id_label_cc = `patientID|labels|chief complaint|`) 

clean_labeled_data <- clean_labeled_data %>%
  separate(col = id_label_cc, into = c('id', 'label', 'cc1', 'cc2'), sep = "[|]", remove = F)

clean_labeled_data$cc1[which(clean_labeled_data$cc1 == "")] = "No CC provided"

clean_labeled_data$cc1 <- as_factor(clean_labeled_data$cc1)
```

## Initial Search Terms For Week 5
```{r Initial Search Terms, message=FALSE}
cc_to_match <- c("short", "fever", "i l i", " ili ", "\\bili\\b","influe", "covid", "HYPOXIA", "DYSPNEA", "SOB", "cough", "DOE")

predicted_covid_1 <- search_assign_by_cc(clean_data, cc_to_match, clean_data$cc1)

covid_prediction_counts_1 <- covid_prediction_count(predicted_covid_1)
covid_prediction_counts_1
```

## Initial Model Performance
```{r first model performance, message=FALSE}

model_1 <- covid_model_comparison(predicted_covid_1, clean_labeled_data)
model_1_table <- covid_model_comparison_table(model_1)
model_1_table
```

```{r}
model_1_evaluated <- evaluation_table(model_1)
model_1_evaluated

plot_confusion_matrix(model_1_evaluated, palette = "Greens")
```

# Error Analysis {#eval}
```{r, message = FALSE}

tp_model_1 <- cc_by_results(model_1, "True Positive")
tn_model_1 <- cc_by_results(model_1, "True Negative") 
fp_model_1 <- cc_by_results(model_1, "False Positive") 
fn_model_1 <- cc_by_results(model_1, "False Negative") 

all_results_model_1 <- bind_rows(tp_model_1, tn_model_1, fp_model_1, fn_model_1)

all_results_model_1_wide <- pivot_wider(all_results_model_1, names_from = results, values_from = count, values_fill = 0)

#Beyond top ten
all_results_model_1_wide_tc <-  model_1 %>% 
  select(id, cc1.x, results) %>% 
  group_by(cc1.x, results) %>% 
  summarise(count = n()) %>% 
  pivot_wider(names_from = results, values_from = count, values_fill = 0) %>% 
  rowwise() %>% 
  mutate(total = sum(`True Positive`, `False Negative`, `True Negative`, `False Positive`)) %>% 
  mutate(condition_present = sum(`True Positive`, `False Negative`)) %>% 
  mutate(cc_pct_positive = round(condition_present/total * 100)) 


```
# Search Terms Revised

## Potential Search Terms

```{r}
cc_to_match_a1 <- c("short", "fever", "i l i", " ili ", "\\bili\\b","influe", "covid", "HYPOXIA", "DYSPNEA", "SOB", "cough", "DOE", "Flu-like symptoms", "chest pain", "hypoxemia", "body aches", "diarrhea", "weakness")

cc_to_match_a2 <- c("fever*","shor", "sob", "hypox", "pnea", "doe", "resp", "pulse ox", "i l i", "\\bili\\b","flu", "cough*", "pneumothorax", "FOUND DOWN", "BODY ACHES", "Chest Pain", "diarrhea", "syncope", "lethargy", "transfer", "CP", "weak", "confusion", "CT", "Tomography", "back pain", "HALLUCINATING")
```



## Analysis pipeline

```{r Analysis pipeline, message=FALSE}

#chosen search term vector 
cc_to_match_example <- cc_to_match_a2

#Do not modify anything below here.  

predicted_covid_example <- search_assign_by_cc(clean_data, cc_to_match_example, clean_data$cc1)

covid_prediction_counts_example <- covid_prediction_count(predicted_covid_example)
covid_prediction_counts_example

model_example <- covid_model_comparison(predicted_covid_example, clean_labeled_data)
model_example_table <- covid_model_comparison_table(model_example)
model_example_table

model_example_evaluated <- evaluation_table(model_example)


plot_confusion_matrix(model_example_evaluated, palette = "Greens")

tp_model_example <- cc_by_results(model_example, "True Positive", 50)
tn_model_example <- cc_by_results(model_example, "True Negative", 50) 
fp_model_example <- cc_by_results(model_example, "False Positive", 50) 
fn_model_example <- cc_by_results(model_example, "False Negative", 50) 

tp_model_example 
tn_model_example  
fp_model_example 
fn_model_example 

all_results <- bind_rows(tp_model_example, tn_model_example, fp_model_example, fn_model_example)

all_results_wide <- pivot_wider(all_results, names_from = results, values_from = count, values_fill = 0)

comparison_new_model_to_model_1 <- bind_rows(model_example_evaluated, model_1_evaluated)


examine_false_negatives <- all_results_wide %>% arrange(desc(`False Negative`))

comparison_new_model_to_model_1

#Beyond top ten
all_results_model_example <-  model_example %>% 
  select(id, cc1.x, results) %>% 
  group_by(cc1.x, results) %>% 
  summarise(count = n()) %>% 
  pivot_wider(names_from = results, values_from = count, values_fill = 0) %>% 
  rowwise() %>% 
  mutate(total = sum(`True Positive`, `False Negative`, `True Negative`, `False Positive`)) %>% 
  mutate(condition_present = sum(`True Positive`, `False Negative`)) %>% 
  mutate(cc_pct_positive = round(condition_present/total * 100)) 

model_by_threshold <-  all_results_model_example %>% 
  filter(cc_pct_positive > 5)
```

# New Model Output {#output}

```{r}
group_predictions_wk6 <- predicted_covid_example %>% select(1,6)
write_csv(group_predictions_wk6, "./analysis/group3_predictions_wk6.csv")

```

## Metric optimization {#metric}

We chose to optimize towards sensitivity. This was chosen partially due to clinical/public health considerations and partially as a response to the constraints of the available data and the context of limited information at the specified point in the pandemic. Casting a broader net felt appropriate in the context of a highly contagious and often asymptomatic respiratory illness. False positives are also comparatively unlikely to lead to patient harm in this context –requiring unnecessary isolation – compared to false negatives, which could be much more harmful to other patients and staff. While optimizing for sensitivity, we did also take care to build in precautions against inclusion of similar strings which had no semantic relevance to the symptoms we intended to screen for. 

Optimizing for sensitivity has serious draw backs however. Once too many people are flagged as likely COVID positive, the usefulness of test decreases. If the point of the test is help allocate resources such as PPE and isolation rooms, having 50% of your patients flag as likely COVID is perhaps more burdensome than helpful. A high sensitivity test for chief complaint still might useful if another screening step can be applied to that group to further discriminate between those more or less likely to have COVID eg a pulse ox reading or age. 

### Session Information
```{r session information}
sessionInfo()
```

## Machine learning time: attempt 1

```{r, evaluate = FALSE}
#First vectorize the text
prep_fun <-  tolower
tok_fun <-  word_tokenizer

covidclass_labels <- read.table("./data_do_not_alter/covidclass_30_percent_labels.tsv",
                                                                header = TRUE, sep = "\t")
sum(is.na(covidclass_labels$labels))
train_ids <- covidclass_labels$patientID[!is.na(covidclass_labels$labels)]
test_ids <- covidclass_labels$patientID[is.na(covidclass_labels$labels)]
head(train_ids)
head(train)
train <- covidclass_labels[train_ids,]
test <- covidclass_labels[test_ids,]
train$chief.complaint <- as.character(train$chief.complaint)
names(covidclass_labels)
covid_train <- itoken(train$chief.complaint,preprocessor = tolower, tokenizer = word_tokenizer,
                      ids = train_ids, progressbar = FALSE)
vocab <- create_vocabulary(covid_train)
# train
# vocab
vectorizer <- vocab_vectorizer(vocab)
dtm_train = create_dtm(covid_train, vectorizer)
#dtm_train

NFOLDS = 4
head(covidclass_labels)
glmnet_classifier = cv.glmnet(x = dtm_train, y = train$labels, 
                              family = 'binomial', 
                              # L1 penalty
                              alpha = 1,
                              # interested in the area under ROC curve
                              type.measure = "auc",
                              # 5-fold cross-validation
                              nfolds = NFOLDS,
                              # high value is less accurate, but has faster training
                              thresh = 1e-3,
                              # again lower number of iterations for faster training
                              maxit = 1e3)
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))

prep_fun <- tolower
it_test = tok_fun(prep_fun(test$chief.complaint))


it_test = itoken(it_test, ids = test$patientID, progressbar = FALSE)
it_test
dtm_test = create_dtm(it_test, vectorizer)
head(test$labels)
preds =  predict(glmnet_classifier, dtm_test, type = 'response')[,1]
#table(test$chief.complaint[preds>0.2])
```

## Second attempt
Source of example code for text2vec: [https://cran.r-project.org/web/packages/text2vec/vignettes/text-vectorization.html#feature_hashing]
```{r}
#Using text2vec example as guide

# Labeled data with week 6 predictions

#Transform to data.table set key to id
week_6_results <- setDT(model_example)

#clean data for processing
#Make chief complaints characters (were factors)
#Rename cc1.x to chief_complaints
week_6_results$cc1.x <- as.character(week_6_results$cc1.x)
week_6_results <- week_6_results %>% 
  rename(chief_complaints = cc1.x)


setkey(week_6_results, id)

#Set seed and divid data with labels into test and train data
set.seed(888)
all_ids_2 <-  week_6_results$id
train_ids_2 <- sample(all_ids_2, 1300)
test_ids_2 <- setdiff(all_ids_2, train_ids_2)

train_2 <-  week_6_results[J(train_ids_2)]
test_2 <-  week_6_results[J(test_ids_2)]

```

### Vectorization
```{r}
# define preprocessing function and tokenization function
prep_fun <-  tolower
tok_fun <-  word_tokenizer

it_train_2 <- itoken(train_2$chief_complaints, 
             preprocessor = prep_fun, 
             tokenizer = tok_fun, 
             ids = train$id, 
             progressbar = FALSE)

vocab_2 <- create_vocabulary(it_train_2)
```

### Create Document Term Matrix
```{r}
vectorizer_2 <- vocab_vectorizer(vocab_2)

dtm_train_2  <-  create_dtm(it_train_2, vectorizer_2)
```

### Fit first model
```{r}
NFOLDS <-  4

glmnet_classifier_2 <-  cv.glmnet(x = dtm_train_2, y = train_2[['covid_status']], 
                              family = 'binomial', 
                              # L1 penalty
                              alpha = 1,
                              # interested in the area under ROC curve
                              type.measure = "auc",
                              # 5-fold cross-validation
                              nfolds = NFOLDS,
                              # high value is less accurate, but has faster training
                              thresh = 1e-3,
                              # again lower number of iterations for faster training
                              maxit = 1e3)

```

```{r}
plot(glmnet_classifier_2)
```

### Trying model on test data
```{r}

it_test_2 <-  tok_fun(prep_fun(test_2$chief_complaints))

it_test_2  <-  itoken(it_test_2, ids = test_2$id, progressbar = FALSE)
         

dtm_test_2 <-  create_dtm(it_test_2, vectorizer_2)

preds_2  <-  predict(glmnet_classifier_2, dtm_test_2, type = 'response')[,1]

```

```{r}
predict_table <-  data.frame(preds_2)
predict_with_id <- rownames_to_column(predict_table, var = "id")
predict_w_id_status <-  predict_with_id %>%
  left_join(clean_labeled_data, by = "id")
```

```{r}
#Source https://rviews.rstudio.com/2019/03/01/some-r-packages-for-roc-curves/
#Week 7 roc curve with package ROCR

pred_week_7 <- prediction(predict_w_id_status$preds_2, predict_w_id_status$label)
perf_week_7 <- performance(pred_week_7,"tpr","fpr")
plot(perf_week_7,colorize=FALSE)
```

```{r}
#Week 7 roc curve with package pROC

pROC_obj <- roc(predict_w_id_status$label, predict_w_id_status$preds_2,
            smoothed = TRUE,
            # arguments for ci
            ci=TRUE, ci.alpha=0.9, stratified=FALSE,
            # arguments for plot
            plot=TRUE, auc.polygon=TRUE, max.auc.polygon=TRUE, grid=TRUE,
            print.auc=TRUE, show.thres=TRUE)


sens.ci <- ci.se(pROC_obj)
plot(sens.ci, type="shape", col="lightblue")
## Warning in plot.ci.se(sens.ci, type = "shape", col = "lightblue"): Low
## definition shape.
plot(sens.ci, type="bars")
```
### Trying model on all data
```{r}
head(clean_labeled_data)

week_7_predictions <-  tok_fun(prep_fun(clean_labeled_data$cc1))

week_7_predictions  <-  itoken(week_7_predictions, ids = week_7_predictions$id, progressbar = FALSE)
         

dtm_week_7_predictions <-  create_dtm(week_7_predictions, vectorizer_2)

week_7_preds  <-  predict(glmnet_classifier_2, dtm_week_7_predictions, type = 'response')[,1]
head(week_7_preds)
```

```{r}
predictions_final_week_7 <-  data.frame(week_7_preds)
predictions_final_week_7 <- rownames_to_column(predictions_final_week_7, var = "id")
head(predictions_final_week_7)
dim(predictions_final_week_7)

predictions_final_week_7 <- predictions_final_week_7 %>% 
  left_join(clean_labeled_data, by = "id")
```

```{r}
plot_wk_7 <- predictions_final_week_7 %>% 
  ggplot(aes(week_7_preds)) + geom_histogram()

plot_wk_7

summary(predictions_final_week_7$week_7_preds)
```
### Predictions for week 7
```{r}
cutoff <- 0.07
```

We choose `r cutoff` as the cutoff for labeling a chief complaint as positive.
```{r}
alarm_sign_sx <- toupper(cc_to_match)
head(predictions_final_week_7)
predictions_final_week_7 <- predictions_final_week_7 %>% 
  mutate(wk_7_pred_label =
           case_when(
             week_7_preds < cutoff ~ 0,
             week_7_preds >= cutoff ~ 1
           ))
predictions_final_week_7$wk_7_pred_label[grep(paste(alarm_sign_sx, collapse = "|"), predictions_final_week_7$cc1)] <- 1

```

### Compare models
```{r}
#Week 6
model_example_evaluated

#Week 7 Training data

week_7_stats <- predictions_final_week_7 %>% 
  filter(label == 0 | label == 1)

week_7_table <- data.frame("target" = c(week_7_stats$label),
                                  "prediction"= c(week_7_stats$wk_7_pred_label)) 

week_7_table_evaluted <- evaluate(week_7_table,
                 target_col = "target",
                 prediction_col = "prediction",
                 type = "binomial")

week_7_table_evaluted
week_7_stats
library(cutpointr)

```

### Error Analysis
```{r}
predictions_final_week_7 <-  rename(predictions_final_week_7, covid_status = label) 
predictions_final_week_7 <-  rename(predictions_final_week_7, covid_prediction = wk_7_pred_label)
predictions_final_week_7 <-  rename(predictions_final_week_7, cc1.x = cc1)

predictions_final_week_7 <-  predictions_final_week_7 %>% 
mutate(results =  case_when(
    covid_status == 0 & covid_prediction == 0 ~ "True Negative",
    covid_status == 1 & covid_prediction == 1 ~ "True Positive",
    covid_status == 0 & covid_prediction == 1 ~ "False Positive",
    covid_status == 1 & covid_prediction == 0 ~ "False Negative"
  )
 )

tp_wk_7 <- cc_by_results(predictions_final_week_7, "True Positive")
tn_wk_7 <- cc_by_results(predictions_final_week_7, "True Negative") 
fp_wk_7 <- cc_by_results(predictions_final_week_7, "False Positive") 
fn_wk_7 <- cc_by_results(predictions_final_week_7, "False Negative") 

(fn_wk_7)

#Calculating youden's J
tpcount <- sum(grepl("True P", predictions_final_week_7$results), na.rm=TRUE)
fpcount <- sum(grepl("False P", predictions_final_week_7$results), na.rm=TRUE)
tncount <- sum(grepl("True N", predictions_final_week_7$results), na.rm=TRUE)
fncount <- sum(grepl("False N", predictions_final_week_7$results), na.rm=TRUE)
youden_stat <- youden(tp = tpcount, fp = fpcount, tn = tncount, fn = fncount)
print(youden_stat)
```


```{r}
#Our COVID prediction is labeled in the column "wk_7_pred_label"
group3_predictions_wk7 <- predictions_final_week_7

write_csv(group3_predictions_wk7, "./analysis/group3_predictions_wk8.csv")
```

